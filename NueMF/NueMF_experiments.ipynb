{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import thunder\n",
    "from NueMF import (\n",
    "    NeuMF,\n",
    "    MovieLensDataModule,\n",
    "    compile_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 14:39:12,570 - NueMF - INFO - Generating test samples...\n"
     ]
    }
   ],
   "source": [
    "gmf_checkpoint = \"/teamspace/studios/this_studio/RecSys/NueMF/checkpoints/gmf-epoch=09.ckpt.ckpt\"\n",
    "mlp_checkpoint = \"/teamspace/studios/this_studio/RecSys/NueMF/checkpoints/mlp-epoch=02.ckpt.ckpt\"\n",
    "nuemf_checkpoint = \"/teamspace/studios/this_studio/RecSys/NueMF/checkpoints/neumf-epoch=02-NeuMF_val_loss=0.00.ckpt\"\n",
    "num_users = 6040\n",
    "num_items = 3706\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    neumf_model = NeuMF.load_from_checkpoint(\n",
    "        nuemf_checkpoint,\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        gmf_checkpoint=gmf_checkpoint,\n",
    "        mlp_checkpoint=mlp_checkpoint,\n",
    "    ).requires_grad_(False).eval()\n",
    "    dm = MovieLensDataModule(batch_size=4096)\n",
    "    dm.setup(\"bench\")\n",
    "    val_loader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "user_ids, item_ids, ratings = batch[\"user_id\"], batch[\"item_id\"], batch[\"rating\"]\n",
    "\n",
    "user_ids = user_ids.to(\"cuda\")\n",
    "item_ids = item_ids.to(\"cuda\")\n",
    "ratings = ratings.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    jit_neumf_model = thunder.jit(neumf_model)\n",
    "\n",
    "jit_neumf_model(user_ids, item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    torch_compiled = torch.compile(neumf_model)\n",
    "\n",
    "torch_compiled(user_ids, item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.77 ms ± 47.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "6.13 ms ± 21.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "6.83 ms ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jit_neumf_model(user_ids, item_ids); torch.cuda.synchronize()\n",
    "%timeit torch_compiled(user_ids, item_ids); torch.cuda.synchronize()\n",
    "%timeit neumf_model(user_ids, item_ids); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Unwrap the actual return value\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def computation(user_ids, item_ids, t_gmf_item_embedding_weight, t_gmf_user_embedding_weight, t_mlp_item_embedding_weight, t_mlp_mlp_layers_0_bias, t_mlp_mlp_layers_0_weight, t_mlp_mlp_layers_1_bias, t_mlp_mlp_layers_1_weight, t_mlp_mlp_layers_4_bias, t_mlp_mlp_layers_4_weight, t_mlp_mlp_layers_7_bias, t_mlp_mlp_layers_7_weight, t_mlp_user_embedding_weight, t_output_layer_bias, t_output_layer_weight):\n",
       "  # user_ids: \"cuda:0 i64[4096]\"\n",
       "  # item_ids: \"cuda:0 i64[4096]\"\n",
       "  # t_gmf_item_embedding_weight: \"cuda:0 f32[3706, 1024]\"\n",
       "  # t_gmf_user_embedding_weight: \"cuda:0 f32[6040, 1024]\"\n",
       "  # t_mlp_item_embedding_weight: \"cuda:0 f32[3706, 1024]\"\n",
       "  # t_mlp_mlp_layers_0_bias: \"cuda:0 f32[1024]\"\n",
       "  # t_mlp_mlp_layers_0_weight: \"cuda:0 f32[1024, 2048]\"\n",
       "  # t_mlp_mlp_layers_1_bias: \"cuda:0 f32[512]\"\n",
       "  # t_mlp_mlp_layers_1_weight: \"cuda:0 f32[512, 1024]\"\n",
       "  # t_mlp_mlp_layers_4_bias: \"cuda:0 f32[256]\"\n",
       "  # t_mlp_mlp_layers_4_weight: \"cuda:0 f32[256, 512]\"\n",
       "  # t_mlp_mlp_layers_7_bias: \"cuda:0 f32[128]\"\n",
       "  # t_mlp_mlp_layers_7_weight: \"cuda:0 f32[128, 256]\"\n",
       "  # t_mlp_user_embedding_weight: \"cuda:0 f32[6040, 1024]\"\n",
       "  # t_output_layer_bias: \"cuda:0 f32[1]\"\n",
       "  # t_output_layer_weight: \"cuda:0 f32[1, 1152]\"\n",
       "  gmf_item_embs = torch.nn.functional.embedding(item_ids, t_gmf_item_embedding_weight, None, None, 2.0, False, False)  # gmf_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "    # gmf_item_embs = ltorch.embedding(item_ids, t_gmf_item_embedding_weight, None, None, 2.0, False, False)  # gmf_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "      # gmf_item_embs = prims.take(t_gmf_item_embedding_weight, item_ids, 0)  # gmf_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "  gmf_user_embs = torch.nn.functional.embedding(user_ids, t_gmf_user_embedding_weight, None, None, 2.0, False, False)  # gmf_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "    # gmf_user_embs = ltorch.embedding(user_ids, t_gmf_user_embedding_weight, None, None, 2.0, False, False)  # gmf_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "      # gmf_user_embs = prims.take(t_gmf_user_embedding_weight, user_ids, 0)  # gmf_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "  mlp_item_embs = torch.nn.functional.embedding(item_ids, t_mlp_item_embedding_weight, None, None, 2.0, False, False)  # mlp_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "    # mlp_item_embs = ltorch.embedding(item_ids, t_mlp_item_embedding_weight, None, None, 2.0, False, False)  # mlp_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "      # mlp_item_embs = prims.take(t_mlp_item_embedding_weight, item_ids, 0)  # mlp_item_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "  mlp_user_embs = torch.nn.functional.embedding(user_ids, t_mlp_user_embedding_weight, None, None, 2.0, False, False)  # mlp_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "    # mlp_user_embs = ltorch.embedding(user_ids, t_mlp_user_embedding_weight, None, None, 2.0, False, False)  # mlp_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "      # mlp_user_embs = prims.take(t_mlp_user_embedding_weight, user_ids, 0)  # mlp_user_embs: \"cuda:0 f32[4096, 1024]\"\n",
       "  [x] = TorchCompile0(mlp_user_embs, mlp_item_embs)\n",
       "    # x = ltorch.cat([mlp_user_embs, mlp_item_embs], 1)  # x: \"cuda:0 f32[4096, 2048]\"\n",
       "      # x = prims.cat([mlp_user_embs, mlp_item_embs], 1)  # x: \"cuda:0 f32[4096, 2048]\"\n",
       "  del mlp_user_embs, mlp_item_embs\n",
       "  input = torch.nn.functional.linear(x, t_mlp_mlp_layers_0_weight, t_mlp_mlp_layers_0_bias)  # input: \"cuda:0 f32[4096, 1024]\"\n",
       "    # input = ltorch.linear(x, t_mlp_mlp_layers_0_weight, t_mlp_mlp_layers_0_bias)  # input: \"cuda:0 f32[4096, 1024]\"\n",
       "      # input = prims.linear(x, t_mlp_mlp_layers_0_weight, t_mlp_mlp_layers_0_bias)  # input: \"cuda:0 f32[4096, 1024]\"\n",
       "  del x\n",
       "  t67 = torch.nn.functional.linear(input, t_mlp_mlp_layers_1_weight, t_mlp_mlp_layers_1_bias)  # t67: \"cuda:0 f32[4096, 512]\"\n",
       "    # t67 = ltorch.linear(input, t_mlp_mlp_layers_1_weight, t_mlp_mlp_layers_1_bias)  # t67: \"cuda:0 f32[4096, 512]\"\n",
       "      # t67 = prims.linear(input, t_mlp_mlp_layers_1_weight, t_mlp_mlp_layers_1_bias)  # t67: \"cuda:0 f32[4096, 512]\"\n",
       "  del input\n",
       "  [t70] = nvFusion0(t67)\n",
       "    # t69 = prims.gt(t67, 0.0)  # t69: \"cuda:0 b8[4096, 512]\"\n",
       "    # t70 = prims.where(t69, t67, 0.0)  # t70: \"cuda:0 f32[4096, 512]\"\n",
       "  del t67\n",
       "  t77 = torch.nn.functional.linear(t70, t_mlp_mlp_layers_4_weight, t_mlp_mlp_layers_4_bias)  # t77: \"cuda:0 f32[4096, 256]\"\n",
       "    # t77 = ltorch.linear(t70, t_mlp_mlp_layers_4_weight, t_mlp_mlp_layers_4_bias)  # t77: \"cuda:0 f32[4096, 256]\"\n",
       "      # t77 = prims.linear(t70, t_mlp_mlp_layers_4_weight, t_mlp_mlp_layers_4_bias)  # t77: \"cuda:0 f32[4096, 256]\"\n",
       "  del t70\n",
       "  [t80] = nvFusion1(t77)\n",
       "    # t79 = prims.gt(t77, 0.0)  # t79: \"cuda:0 b8[4096, 256]\"\n",
       "    # t80 = prims.where(t79, t77, 0.0)  # t80: \"cuda:0 f32[4096, 256]\"\n",
       "  del t77\n",
       "  t87 = torch.nn.functional.linear(t80, t_mlp_mlp_layers_7_weight, t_mlp_mlp_layers_7_bias)  # t87: \"cuda:0 f32[4096, 128]\"\n",
       "    # t87 = ltorch.linear(t80, t_mlp_mlp_layers_7_weight, t_mlp_mlp_layers_7_bias)  # t87: \"cuda:0 f32[4096, 128]\"\n",
       "      # t87 = prims.linear(t80, t_mlp_mlp_layers_7_weight, t_mlp_mlp_layers_7_bias)  # t87: \"cuda:0 f32[4096, 128]\"\n",
       "  del t80\n",
       "  [mlp_vector] = nvFusion2(t87)\n",
       "    # t89 = prims.gt(t87, 0.0)  # t89: \"cuda:0 b8[4096, 128]\"\n",
       "    # mlp_vector = prims.where(t89, t87, 0.0)  # mlp_vector: \"cuda:0 f32[4096, 128]\"\n",
       "  del t87\n",
       "  [concat] = TorchCompile1(gmf_user_embs, gmf_item_embs, mlp_vector)\n",
       "    # gmf_vector = ltorch.mul(gmf_user_embs, gmf_item_embs)  # gmf_vector: \"cuda:0 f32[4096, 1024]\"\n",
       "      # gmf_vector = prims.mul(gmf_user_embs, gmf_item_embs)  # gmf_vector: \"cuda:0 f32[4096, 1024]\"\n",
       "    # concat = ltorch.cat([gmf_vector, mlp_vector], 1)  # concat: \"cuda:0 f32[4096, 1152]\"\n",
       "      # concat = prims.cat([gmf_vector, mlp_vector], 1)  # concat: \"cuda:0 f32[4096, 1152]\"\n",
       "  del gmf_user_embs, gmf_item_embs, mlp_vector\n",
       "  output = torch.nn.functional.linear(concat, t_output_layer_weight, t_output_layer_bias)  # output: \"cuda:0 f32[4096, 1]\"\n",
       "    # output = ltorch.linear(concat, t_output_layer_weight, t_output_layer_bias)  # output: \"cuda:0 f32[4096, 1]\"\n",
       "      # output = prims.linear(concat, t_output_layer_weight, t_output_layer_bias)  # output: \"cuda:0 f32[4096, 1]\"\n",
       "  del concat\n",
       "  [t102] = nvFusion3(output)\n",
       "    # t99 = prims.neg(output)  # t99: \"cuda:0 f32[4096, 1]\"\n",
       "    # t100 = prims.exp(t99)  # t100: \"cuda:0 f32[4096, 1]\"\n",
       "    # t101 = prims.add(1.0, t100)  # t101: \"cuda:0 f32[4096, 1]\"\n",
       "    # t102 = prims.reciprocal(t101)  # t102: \"cuda:0 f32[4096, 1]\"\n",
       "  del output\n",
       "  return t102"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(jit_neumf_model)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.477ms        53.44%       2.477ms       1.239ms             2  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     858.594us        18.52%     858.594us     429.297us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     587.979us        12.69%     587.979us     146.995us             4  \n",
      "                                                triton_         0.00%       0.000us         0.00%       0.000us       0.000us     496.431us        10.71%     496.431us     248.216us             2  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      87.549us         1.89%      87.549us      87.549us             1  \n",
      "(anonymous namespace)::nvfuser_pointwise_f4_c1_r0_g0...         0.00%       0.000us         0.00%       0.000us       0.000us      72.254us         1.56%      72.254us      72.254us             1  \n",
      "(anonymous namespace)::nvfuser_pointwise_f5_c1_r0_g0...         0.00%       0.000us         0.00%       0.000us       0.000us      37.151us         0.80%      37.151us      37.151us             1  \n",
      "(anonymous namespace)::nvfuser_pointwise_f6_c1_r0_g0...         0.00%       0.000us         0.00%       0.000us       0.000us      11.840us         0.26%      11.840us      11.840us             1  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.560us         0.06%       2.560us       2.560us             1  \n",
      "(anonymous namespace)::nvfuser_pointwise_f7_c1_r0_g0...         0.00%       0.000us         0.00%       0.000us       0.000us       1.984us         0.04%       1.984us       1.984us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.664us         0.04%       1.664us       0.555us             3  \n",
      "                                       cudaLaunchKernel         4.09%     129.126us         4.09%     129.126us      12.913us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "                                         cuLaunchKernel         2.12%      67.003us         2.12%      67.003us      11.167us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                        cudaMemsetAsync         0.85%      26.940us         0.85%      26.940us       8.980us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.25%       7.779us         0.25%       7.779us       1.945us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                  cudaDeviceSynchronize        92.68%       2.923ms        92.68%       2.923ms       2.923ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.154ms\n",
      "Self CUDA time total: 4.635ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"thunder\"):\n",
    "        out = jit_neumf_model(user_ids, item_ids)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.472ms        50.84%       2.472ms       1.236ms             2  \n",
      "                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     859.623us        17.68%     859.623us     429.811us             2  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us     586.161us        12.06%     586.161us     146.540us             4  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     519.954us        10.70%     519.954us     259.977us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     209.338us         4.31%     209.338us     209.338us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     119.006us         2.45%     119.006us      39.669us             3  \n",
      "void gemv2T_kernel_val<int, int, float, float, float...         0.00%       0.000us         0.00%       0.000us       0.000us      88.605us         1.82%      88.605us      88.605us             1  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         0.05%       2.656us       2.656us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.336us         0.05%       2.336us       0.779us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.176us         0.04%       2.176us       2.176us             1  \n",
      "                                       cudaLaunchKernel         4.99%     201.924us         4.99%     201.924us      11.878us       0.000us         0.00%       0.000us       0.000us            17  \n",
      "                                        cudaMemsetAsync         0.56%      22.769us         0.56%      22.769us       7.590us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.16%       6.304us         0.16%       6.304us       1.576us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                  cudaDeviceSynchronize        94.29%       3.816ms        94.29%       3.816ms       3.816ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.047ms\n",
      "Self CUDA time total: 4.861ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"eager\"):\n",
    "        out = neumf_model(user_ids, item_ids)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
